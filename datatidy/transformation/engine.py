"""Main transformation engine for data processing."""

from typing import Any, Dict, List, Optional, Union
import pandas as pd
import numpy as np
import re
from .expressions import ExpressionParser
from .column_operations import AdvancedColumnTransformer
from .dependency_resolver import ColumnExecutionEngine


class ValidationError(Exception):
    """Exception raised when data validation fails."""
    pass


class TransformationEngine:
    """Engine for applying transformations and validations to data."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize transformation engine with configuration."""
        self.config = config
        self.expression_parser = ExpressionParser()
        self.column_transformer = AdvancedColumnTransformer(
            self.expression_parser.safe_parser.SAFE_FUNCTIONS
        )
        self.execution_engine = ColumnExecutionEngine()
        self.errors: List[Dict[str, Any]] = []
        self.max_errors = config.get("global_settings", {}).get("max_errors", 100)
        self.ignore_errors = config.get("global_settings", {}).get("ignore_errors", False)
        self.execution_plan: Optional[Dict[str, Any]] = None
    
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply all transformations to the DataFrame with dependency resolution."""
        self.errors.clear()
        
        output_columns = self.config["output"]["columns"]
        input_columns = set(df.columns)
        
        # Plan execution with dependency resolution
        try:
            self.execution_plan = self.execution_engine.plan_execution(
                output_columns, input_columns
            )
            
            if self.config.get("global_settings", {}).get("show_execution_plan", False):
                self._log_execution_plan()
        
        except Exception as e:
            self._handle_error(f"Execution planning failed: {str(e)}")
            # Fall back to original order if planning fails
            execution_order = list(output_columns.keys())
            self.execution_plan = {
                'execution_order': execution_order,
                'final_columns': execution_order,
                'interim_columns': []
            }
        
        # Start with input DataFrame
        result_df = df.copy()
        
        # Process columns in dependency order
        execution_order = self.execution_plan['execution_order']
        
        for column_name in execution_order:
            column_config = output_columns[column_name]
            try:
                # Process the column and add to result
                result_df[column_name] = self._process_column(result_df, column_name, column_config)
                
                # Log progress for debugging
                if self.config.get("global_settings", {}).get("verbose", False):
                    print(f"âœ“ Processed column: {column_name}")
                    
            except Exception as e:
                self._handle_error(f"Error processing column '{column_name}': {str(e)}")
                if not self.ignore_errors:
                    continue
        
        # Filter out interim columns from final result
        final_columns = self.execution_plan.get('final_columns', list(output_columns.keys()))
        
        # Keep input columns and final output columns
        columns_to_keep = list(input_columns) + [col for col in final_columns if col in result_df.columns]
        
        # Remove duplicates while preserving order
        seen = set()
        columns_to_keep = [col for col in columns_to_keep if not (col in seen or seen.add(col))]
        
        # If user specified to only keep output columns, filter input columns too
        if self.config["output"].get("only_output_columns", False):
            columns_to_keep = [col for col in final_columns if col in result_df.columns]
        
        result_df = result_df[columns_to_keep]
        
        # Apply filters
        if "filters" in self.config["output"]:
            result_df = self._apply_filters(result_df)
        
        # Apply sorting
        if "sort" in self.config["output"]:
            result_df = self._apply_sorting(result_df)
        
        return result_df
    
    def _log_execution_plan(self) -> None:
        """Log the execution plan for debugging."""
        if not self.execution_plan:
            return
            
        print("=== EXECUTION PLAN ===")
        print(f"Total columns to process: {self.execution_plan['total_steps']}")
        print(f"Execution order: {' -> '.join(self.execution_plan['execution_order'])}")
        print(f"Interim columns: {self.execution_plan['interim_columns']}")
        print(f"Final columns: {self.execution_plan['final_columns']}")
        
        # Show dependency information
        dep_info = self.execution_plan.get('dependency_info', {})
        if dep_info.get('dependency_graph'):
            print("\nDependencies:")
            for col, deps in dep_info['dependency_graph'].items():
                if deps:
                    print(f"  {col} depends on: {', '.join(sorted(deps))}")
        print("======================")
    
    def _process_column(self, df: pd.DataFrame, column_name: str, column_config: Dict[str, Any]) -> pd.Series:
        """Process a single column according to its configuration."""
        # Get source data
        if "operations" in column_config:
            # Advanced column operations (map/reduce/filter)
            source = column_config.get("source", column_name)
            if source in df.columns:
                series = df[source].copy()
            else:
                # Try to evaluate as expression first
                try:
                    series = self.expression_parser.evaluate_vectorized(source, df)
                except Exception:
                    raise ValueError(f"Source column '{source}' not found")
            
            # Apply advanced operations
            operations = column_config["operations"]
            series = self.column_transformer.apply_operations(
                series, operations, dataframe=df
            )
            
        elif "transformation" in column_config:
            # Apply transformation expression
            transformation = column_config["transformation"]
            series = self.expression_parser.evaluate_vectorized(transformation, df)
        else:
            # Simple column mapping
            source = column_config.get("source", column_name)
            if source in df.columns:
                series = df[source].copy()
            else:
                # Try to evaluate as expression
                try:
                    series = self.expression_parser.evaluate_vectorized(source, df)
                except Exception:
                    raise ValueError(f"Source column '{source}' not found")
        
        # Apply data type conversion
        target_type = column_config.get("type", "string")
        series = self._convert_type(series, target_type, column_config.get("format"))
        
        # Apply default values
        if "default" in column_config:
            default_value = column_config["default"]
            series = series.fillna(default_value)
        
        # Validate data
        if "validation" in column_config:
            series = self._validate_column(series, column_name, column_config["validation"])
        
        return series
    
    def _convert_type(self, series: pd.Series, target_type: str, format_str: Optional[str] = None) -> pd.Series:
        """Convert series to target data type."""
        try:
            if target_type == "string":
                return series.astype(str)
            elif target_type == "int":
                return pd.to_numeric(series, errors='coerce').astype('Int64')
            elif target_type == "float":
                return pd.to_numeric(series, errors='coerce')
            elif target_type == "bool":
                return series.astype(bool)
            elif target_type == "datetime":
                if format_str:
                    return pd.to_datetime(series, format=format_str, errors='coerce')
                else:
                    return pd.to_datetime(series, errors='coerce')
            else:
                raise ValueError(f"Unsupported data type: {target_type}")
        except Exception as e:
            raise ValueError(f"Type conversion failed: {str(e)}")
    
    def _validate_column(self, series: pd.Series, column_name: str, validation_rules: Dict[str, Any]) -> pd.Series:
        """Validate column data according to validation rules."""
        errors = []
        
        # Required validation
        if validation_rules.get("required", True):
            null_mask = series.isna()
            if null_mask.any():
                null_indices = series[null_mask].index.tolist()
                errors.append(f"Required field '{column_name}' has null values at indices: {null_indices}")
        
        # Nullable validation
        if not validation_rules.get("nullable", False):
            null_mask = series.isna()
            if null_mask.any():
                null_indices = series[null_mask].index.tolist()
                errors.append(f"Non-nullable field '{column_name}' has null values at indices: {null_indices}")
        
        # Numeric validations
        if "min_value" in validation_rules:
            min_val = validation_rules["min_value"]
            invalid_mask = series < min_val
            if invalid_mask.any():
                invalid_indices = series[invalid_mask].index.tolist()
                errors.append(f"Field '{column_name}' has values below minimum ({min_val}) at indices: {invalid_indices}")
        
        if "max_value" in validation_rules:
            max_val = validation_rules["max_value"]
            invalid_mask = series > max_val
            if invalid_mask.any():
                invalid_indices = series[invalid_mask].index.tolist()
                errors.append(f"Field '{column_name}' has values above maximum ({max_val}) at indices: {invalid_indices}")
        
        # String length validations
        if "min_length" in validation_rules:
            min_len = validation_rules["min_length"]
            string_series = series.astype(str)
            invalid_mask = string_series.str.len() < min_len
            if invalid_mask.any():
                invalid_indices = series[invalid_mask].index.tolist()
                errors.append(f"Field '{column_name}' has values shorter than minimum length ({min_len}) at indices: {invalid_indices}")
        
        if "max_length" in validation_rules:
            max_len = validation_rules["max_length"]
            string_series = series.astype(str)
            invalid_mask = string_series.str.len() > max_len
            if invalid_mask.any():
                invalid_indices = series[invalid_mask].index.tolist()
                errors.append(f"Field '{column_name}' has values longer than maximum length ({max_len}) at indices: {invalid_indices}")
        
        # Pattern validation
        if "pattern" in validation_rules:
            pattern = validation_rules["pattern"]
            string_series = series.astype(str)
            try:
                invalid_mask = ~string_series.str.match(pattern, na=False)
                if invalid_mask.any():
                    invalid_indices = series[invalid_mask].index.tolist()
                    errors.append(f"Field '{column_name}' has values not matching pattern '{pattern}' at indices: {invalid_indices}")
            except re.error as e:
                errors.append(f"Invalid regex pattern for field '{column_name}': {str(e)}")
        
        # Allowed values validation
        if "allowed_values" in validation_rules:
            allowed_values = validation_rules["allowed_values"]
            invalid_mask = ~series.isin(allowed_values)
            if invalid_mask.any():
                invalid_indices = series[invalid_mask].index.tolist()
                errors.append(f"Field '{column_name}' has values not in allowed list {allowed_values} at indices: {invalid_indices}")
        
        # Handle validation errors
        for error in errors:
            self._handle_error(error)
        
        return series
    
    def _apply_filters(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply filters to the DataFrame."""
        filters = self.config["output"]["filters"]
        
        for filter_config in filters:
            condition = filter_config["condition"]
            action = filter_config.get("action", "keep")
            
            try:
                # Evaluate condition
                mask = self.expression_parser.evaluate_vectorized(condition, df)
                
                if action == "keep":
                    df = df[mask]
                elif action == "remove":
                    df = df[~mask]
                
            except Exception as e:
                self._handle_error(f"Error applying filter '{condition}': {str(e)}")
        
        return df
    
    def _apply_sorting(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply sorting to the DataFrame."""
        sort_configs = self.config["output"]["sort"]
        
        sort_columns = []
        sort_ascending = []
        
        for sort_config in sort_configs:
            column = sort_config["column"]
            ascending = sort_config.get("ascending", True)
            
            if column in df.columns:
                sort_columns.append(column)
                sort_ascending.append(ascending)
            else:
                self._handle_error(f"Sort column '{column}' not found in DataFrame")
        
        if sort_columns:
            try:
                df = df.sort_values(sort_columns, ascending=sort_ascending)
            except Exception as e:
                self._handle_error(f"Error sorting DataFrame: {str(e)}")
        
        return df
    
    def _handle_error(self, error_message: str) -> None:
        """Handle validation and processing errors."""
        self.errors.append({
            "message": error_message,
            "timestamp": pd.Timestamp.now()
        })
        
        if len(self.errors) >= self.max_errors:
            raise ValidationError(f"Maximum number of errors ({self.max_errors}) exceeded")
        
        if not self.ignore_errors:
            raise ValidationError(error_message)
    
    def get_errors(self) -> List[Dict[str, Any]]:
        """Get list of validation errors."""
        return self.errors.copy()
    
    def has_errors(self) -> bool:
        """Check if there are any validation errors."""
        return len(self.errors) > 0