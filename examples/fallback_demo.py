#!/usr/bin/env python3
"""
Demo script showing DataTidy's enhanced fallback processing capabilities.

This script demonstrates:
1. Partial processing mode
2. Enhanced error logging and categorization
3. Data quality comparison
4. Processing recommendations
5. Fallback query integration

Run with: python examples/fallback_demo.py
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys

# Add the parent directory to the path to import datatidy
sys.path.insert(0, str(Path(__file__).parent.parent))

from datatidy import DataTidy
from datatidy.fallback.metrics import DataQualityMetrics


def create_sample_data():
    """Create sample data with various data quality issues."""
    np.random.seed(42)

    data = pd.DataFrame(
        {
            "id": range(1, 101),
            "name": [
                f"Facility_{i}" if i % 10 != 0 else None for i in range(1, 101)
            ],  # 10% nulls
            "debt_to_income": np.random.normal(2.5, 1.0, 100),
            "leverage_ratio": np.random.normal(1.8, 0.5, 100),
            "existing_risk_level": np.random.choice(["LOW", "MEDIUM", "HIGH"], 100),
            "amount": np.random.normal(1000000, 500000, 100),
        }
    )

    # Introduce some problematic values
    data.loc[data.index % 15 == 0, "debt_to_income"] = None  # More nulls
    data.loc[data.index % 20 == 0, "leverage_ratio"] = None  # More nulls
    data.loc[data.index < 5, "amount"] = -100000  # Negative values

    return data


def create_problematic_config():
    """Create a configuration that will have some failures."""
    return {
        "output": {
            "columns": {
                "facility_id": {
                    "source": "id",
                    "type": "int",
                    "validation": {"required": True},
                },
                "facility_name": {
                    "source": "name",
                    "type": "string",
                    "validation": {"required": True},  # This will fail due to nulls
                },
                "cash_flow_leverage": {
                    "transformation": "debt_to_income * leverage_ratio",
                    "type": "float",
                    "validation": {
                        "required": True,  # This will fail due to nulls
                        "min_value": 0,
                        "max_value": 20,
                    },
                },
                "risk_category": {
                    "transformation": "np.where(cash_flow_leverage > 5, 'HIGH', np.where(cash_flow_leverage > 2, 'MEDIUM', 'LOW'))",
                    "type": "string",
                    "validation": {"allowed_values": ["LOW", "MEDIUM", "HIGH"]},
                },
                "processed_amount": {
                    "transformation": "amount * 1.1",
                    "type": "float",
                    "validation": {
                        "min_value": 0
                    },  # This will fail for negative amounts
                },
            }
        },
        "global_settings": {
            "processing_mode": "partial",
            "enable_partial_processing": True,
            "enable_fallback": True,
            "max_column_failures": 10,
            "failure_threshold": 0.4,
            "fallback_transformations": {
                "facility_name": {"type": "default_value", "value": "Unknown Facility"},
                "cash_flow_leverage": {
                    "type": "basic_calculation",
                    "operation": "mean",
                    "source": "debt_to_income",
                },
                "processed_amount": {"type": "copy_column", "source": "amount"},
            },
            "verbose": True,
        },
    }


def fallback_database_query():
    """Simulate a fallback database query."""
    print("üîÑ Executing fallback database query...")

    # Simulate cleaner data from database
    return pd.DataFrame(
        {
            "facility_id": range(1, 101),
            "facility_name": [f"DB_Facility_{i}" for i in range(1, 101)],
            "risk_level": np.random.choice(["LOW", "MEDIUM", "HIGH"], 100),
            "total_amount": np.random.normal(1000000, 300000, 100),
        }
    )


def main():
    """Run the fallback processing demo."""
    print("=" * 60)
    print("üöÄ DataTidy Fallback Processing Demo")
    print("=" * 60)

    # Create sample data with quality issues
    print("\nüìä Creating sample data with quality issues...")
    sample_data = create_sample_data()
    print(f"Created dataset with {len(sample_data)} rows")
    print(f"Null values per column:")
    for col, nulls in sample_data.isnull().sum().items():
        if nulls > 0:
            print(f"  {col}: {nulls} nulls ({nulls/len(sample_data)*100:.1f}%)")

    # Create configuration with potential issues
    config = create_problematic_config()

    # Initialize DataTidy
    print("\nüîß Initializing DataTidy with problematic configuration...")
    dt = DataTidy()
    dt.load_config_from_string("# Loaded from dict")
    dt.config = config
    dt.logger = dt.logger or dt.EnhancedLogger()
    dt.fallback_processor = dt.FallbackProcessor(config, dt.logger)
    dt.transformation_engine = dt.TransformationEngine(config)

    # Demo 1: Strict mode (will fail)
    print("\n" + "=" * 50)
    print("üìã DEMO 1: Strict Mode Processing")
    print("=" * 50)

    dt.set_processing_mode("strict")
    try:
        result_strict = dt.process_data_with_fallback(sample_data)
        print("‚úÖ Strict mode succeeded (unexpected!)")
    except Exception as e:
        print(f"‚ùå Strict mode failed as expected: {str(e)[:100]}...")

    # Demo 2: Partial mode
    print("\n" + "=" * 50)
    print("üìã DEMO 2: Partial Mode Processing")
    print("=" * 50)

    dt.set_processing_mode("partial")
    result_partial = dt.process_data_with_fallback(sample_data)

    print(f"\nüìà Processing Results:")
    print(f"   Success: {'‚úÖ' if result_partial.success else '‚ùå'}")
    print(f"   Successful columns: {len(result_partial.successful_columns)}")
    print(f"   Failed columns: {len(result_partial.failed_columns)}")
    print(f"   Error count: {len(result_partial.error_log)}")

    if result_partial.successful_columns:
        print(f"   ‚úÖ Successful: {', '.join(result_partial.successful_columns)}")
    if result_partial.failed_columns:
        print(f"   ‚ùå Failed: {', '.join(result_partial.failed_columns)}")

    # Show error details
    if result_partial.error_log:
        print(f"\n‚ö†Ô∏è  Error Details (showing first 3):")
        for i, error in enumerate(result_partial.error_log[:3]):
            print(f"   {i+1}. {error['column']}: {error['error_message']}")

    # Show recommendations
    print(f"\nüí° Processing Recommendations:")
    recommendations = dt.get_processing_recommendations()
    for rec in recommendations[:5]:
        print(f"   {rec}")

    # Demo 3: Fallback mode with external query
    print("\n" + "=" * 50)
    print("üìã DEMO 3: Fallback Mode with External Query")
    print("=" * 50)

    dt.set_processing_mode("fallback")
    result_fallback = dt.process_data_with_fallback(
        sample_data, fallback_query_func=fallback_database_query
    )

    print(f"\nüìà Fallback Results:")
    print(f"   Success: {'‚úÖ' if result_fallback.success else '‚ùå'}")
    print(f"   Fallback used: {'‚úÖ' if result_fallback.fallback_used else '‚ùå'}")
    print(f"   Result shape: {result_fallback.data.shape}")
    print(f"   Columns: {list(result_fallback.data.columns)}")

    # Demo 4: Data Quality Comparison
    if result_partial.success and result_fallback.success:
        print("\n" + "=" * 50)
        print("üìã DEMO 4: Data Quality Comparison")
        print("=" * 50)

        # Compare partial processing result with fallback
        comparison = DataQualityMetrics.compare_results(
            result_partial.data,
            result_fallback.data,
            result_partial.processing_time,
            result_fallback.processing_time,
        )

        DataQualityMetrics.print_comparison_summary(comparison)

    # Demo 5: Error Log Export
    print("\n" + "=" * 50)
    print("üìã DEMO 5: Error Log Export")
    print("=" * 50)

    error_file = "fallback_demo_errors.json"
    dt.export_error_log(error_file)
    print(f"üìÅ Error log exported to: {error_file}")

    # Show processing summary
    print("\nüìä Final Processing Summary:")
    summary = dt.get_processing_summary()
    for key, value in summary.items():
        print(f"   {key}: {value}")

    print("\n" + "=" * 60)
    print("‚úÖ Fallback Processing Demo Complete!")
    print("=" * 60)
    print("\nüí° Key Benefits Demonstrated:")
    print("   ‚Ä¢ Graceful degradation when columns fail")
    print("   ‚Ä¢ Detailed error categorization and logging")
    print("   ‚Ä¢ Automatic fallback to database queries")
    print("   ‚Ä¢ Data quality comparison and metrics")
    print("   ‚Ä¢ Actionable recommendations for improvements")
    print("\nüéØ This ensures your applications maintain high availability")
    print("   while leveraging DataTidy's advanced processing when possible!")


if __name__ == "__main__":
    main()
